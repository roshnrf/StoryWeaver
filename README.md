# üéØ StoryWeaver: AI-Powered Speech Therapy for Children

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue)](https://www.python.org/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28%2B-red)](https://streamlit.io/)
[![Whisper](https://img.shields.io/badge/OpenAI-Whisper-green)](https://github.com/openai/whisper)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)
[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-brightgreen.svg)](https://github.com/roshnrf/storyweaver-speech-therapy/graphs/commit-activity)

> **A Generative AI Platform for Collaborative Speech Therapy**  
> Helping children (ages 4-6) improve English pronunciation through interactive storytelling

**Authors:** Roshan A Rauof , Reem Fariha 

---

## üìñ Table of Contents

- [Overview](##overview)
- [Key Features](#key-features)
- [Architecture](https://github.com/roshnrf/StoryWeaver?tab=readme-ov-file#%EF%B8%8F-architecture)
- [Installation](#installation)
- [Usage](#usage)
- [Project Structure](#project-structure)
- [Results & Analysis](#results--analysis)
- [Technologies Used](##technologies-used)
- [Ethics & Privacy](#ethics--privacy)
- [Future Enhancements](#future-enhancements)
- [Contributing](#contributing)
- [License](#license)
- [Contact](#contact)

---

## üéØ Overview

**StoryWeaver** is an intelligent speech therapy platform that makes pronunciation practice engaging for young children through personalized storytelling. The system:

1. Shows a picture to the child
2. Records and transcribes their description
3. Detects articulation, grammar, and vocabulary errors
4. Generates a custom practice story incorporating the difficult words
5. Guides line-by-line practice with real-time feedback
6. Tracks progress across sessions

### üé• Demo

![System Architecture](docs/architecture_diagram.png)

---

## ‚ú® Key Features

### üé§ **Speech Recognition**
- Real-time audio recording and transcription using OpenAI Whisper
- Optimized for children's voices and Indian-accented English
- High accuracy (85-95%) on diverse pronunciations

### üîç **Intelligent Error Detection**
- **Grammar errors**: Article usage, verb tense, sentence structure
- **Vocabulary errors**: Word choice and appropriateness
- **Articulation errors**: Phonetic patterns (th, s, r sounds)
- Powered by Google Gemini 2.5 Flash

### üìö **Personalized Story Generation**
- Creates age-appropriate stories (4-6 years)
- Naturally incorporates challenging words for practice
- Breaks stories into manageable sections
- Engaging narratives around familiar topics

### üéØ **Practice & Feedback**
- Line-by-line guided reading practice
- Text-to-speech model pronunciation (gTTS)
- Real-time similarity scoring (60-100%)
- Visual highlighting of focus words
- Encouraging feedback system

### üìä **Progress Tracking**
- Session-by-session metrics
- Accuracy trends over time
- Error type distribution
- Attempt history per section
- JSON-based local storage

---

## üèóÔ∏è Architecture

### System Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    1. SESSION INITIALIZATION                     ‚îÇ
‚îÇ              Display picture ‚Üí Prompt description                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      2. AUDIO RECORDING                          ‚îÇ
‚îÇ                  Child describes the picture                     ‚îÇ
‚îÇ                   (st_audiorec widget)                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              3. TRANSCRIPTION & ERROR DETECTION                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê          ‚îÇ
‚îÇ  ‚îÇ  Whisper ASR       ‚îÇ        ‚îÇ  Google Gemini      ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ  Speech ‚Üí Text     ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ  Error Analysis     ‚îÇ          ‚îÇ
‚îÇ  ‚îÇ  (base model)      ‚îÇ        ‚îÇ  (JSON output)      ‚îÇ          ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   4. SESSION MANAGEMENT                          ‚îÇ
‚îÇ              Store errors, transcript, metadata                  ‚îÇ
‚îÇ                  (progress_data.json)                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    5. STORY GENERATION                           ‚îÇ
‚îÇ                    Google Gemini Models                          ‚îÇ
‚îÇ     Create personalized story with error-focused words          ‚îÇ
‚îÇ              Split into practice sections                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    6. PRACTICE SESSION                           ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê       ‚îÇ
‚îÇ  ‚îÇ  gTTS Model  ‚îÇ‚Üí ‚îÇ Child Reads    ‚îÇ‚Üí ‚îÇ Similarity    ‚îÇ       ‚îÇ
‚îÇ  ‚îÇ Pronunciation‚îÇ  ‚îÇ & Records      ‚îÇ  ‚îÇ Scoring       ‚îÇ       ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò       ‚îÇ
‚îÇ           ‚îÇ               ‚îÇ                    ‚îÇ                ‚îÇ
‚îÇ           ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                ‚îÇ
‚îÇ                    Repeat until 80%+ accuracy                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                     7. SAVE PROGRESS                             ‚îÇ
‚îÇ        Update metrics: accuracy, attempts, completion            ‚îÇ
‚îÇ              Generate session summary report                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Technical Architecture

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **Frontend** | Streamlit | Interactive UI, audio recording, visualization |
| **ASR** | OpenAI Whisper | Speech-to-text transcription |
| **NLP/AI** | Google Gemini 2.5 Flash | Error detection, story generation |
| **TTS** | gTTS | Text-to-speech for model pronunciation |
| **Storage** | JSON (local) | Session data, progress tracking |

---

## üöÄ Installation

### Prerequisites

- Python 3.8 or higher
- 8GB RAM minimum (16GB recommended)
- Internet connection (for AI models)
- Microphone for audio recording

### Step 1: Clone Repository

```bash
git clone https://github.com/roshnrf/storyweaver.git
cd storyweaver
```

### Step 2: Create Virtual Environment

```bash
# Create virtual environment
python -m venv venv

# Activate (Mac/Linux)
source venv/bin/activate

# Activate (Windows)
venv\Scripts\activate
```

### Step 3: Install Dependencies

```bash
pip install -r requirements.txt
```

### Step 4: Set Up Environment Variables

```bash
# Copy example environment file
cp .env.example .env

# Edit .env and add your Google API key
# Get key from: https://makersuite.google.com/app/apikey
```

### Step 5: Add Picture Prompts

```bash
# Create pictures folder if not exists
mkdir -p pictures

# Add images (dog.jpg, cat.jpg, dolphin.jpg, car.jpg, rainbow.jpg)
# Or use the provided sample images in pictures/
```

### Step 6: Run the Application

```bash
streamlit run app.py
```

The app will open in your browser at `http://localhost:8501`

---

## üíª Usage

### Quick Start Guide

1. **Start Session**: The app displays a random picture
2. **Record Description**: Click the microphone icon and describe what you see
3. **Review Errors**: System shows detected errors in a table
4. **Read Story**: Practice the generated story line by line
5. **Get Feedback**: Receive accuracy scores and tips
6. **Track Progress**: View your improvement in the sidebar

### For Parents/Therapists

```python
# Customize settings in config.py
WHISPER_MODEL = "base"  # Options: tiny, base, small, medium, large
ACCURACY_THRESHOLD = 80  # Minimum accuracy to proceed (60-100)
ENABLE_GOOGLE_GENAI = True  # Set False to disable AI features
```

### Sample Session Flow

```
Child sees: [Picture of a Dog]
Child says: "This dog have happy face"

System detects:
- Grammar: "have" ‚Üí "has"
- Articulation: "Dis" ‚Üí "This" (th sound)

Generated story:
"This happy dog has a friendly face. | 
 This dog has soft brown fur. |
 The dog has four speedy paws."

Practice outcome:
Section 1: 94% accuracy ‚úÖ
Section 2: 87% accuracy ‚úÖ
Section 3: 96% accuracy ‚úÖ

Session complete! üéâ
```

---

## üìÅ Project Structure

```
storyweaver/
‚îú‚îÄ‚îÄ app.py                      # Main Streamlit application
‚îú‚îÄ‚îÄ config.py                   # Configuration settings
‚îú‚îÄ‚îÄ requirements.txt            # Python dependencies
‚îú‚îÄ‚îÄ .env.example               # Environment variables template
‚îú‚îÄ‚îÄ .gitignore                 # Git ignore rules
‚îú‚îÄ‚îÄ LICENSE                    # MIT License
‚îú‚îÄ‚îÄ README.md                  # This file
‚îÇ
‚îú‚îÄ‚îÄ pictures/                  # Picture prompts (gitignored)
‚îÇ   ‚îú‚îÄ‚îÄ dog.jpg
‚îÇ   ‚îú‚îÄ‚îÄ cat.jpg
‚îÇ   ‚îú‚îÄ‚îÄ dolphin.jpg
‚îÇ   ‚îú‚îÄ‚îÄ car.jpg
‚îÇ   ‚îî‚îÄ‚îÄ rainbow.jpg
‚îÇ
‚îú‚îÄ‚îÄ progress_data.json         # Session history (gitignored)
‚îÇ
‚îú‚îÄ‚îÄ docs/                      # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ architecture_diagram.png
‚îÇ   ‚îú‚îÄ‚îÄ REPORT.pdf
‚îÇ   ‚îú‚îÄ‚îÄ screenshots/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ landing_page.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ recording_interface.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ error_detection.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ story_practice.png
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ progress_dashboard.png
‚îÇ   ‚îî‚îÄ‚îÄ presentation.pptx
‚îÇ
‚îú‚îÄ‚îÄ tests/                     # Unit tests (future)
‚îÇ   ‚îî‚îÄ‚îÄ test_app.py
‚îÇ
‚îî‚îÄ‚îÄ ETHICS.md                  # Ethics & privacy policy
```

---

## üìä Results & Analysis

### Pilot Study Overview

**Participants:** N = 2 children (ages 4 and 6)  
**Duration:** 1 week  
**Sessions:** 6 total sessions

> ‚ö†Ô∏è **Important:** These are preliminary pilot results and are not statistically generalizable. Full study required for validation.

### Performance Metrics

| Metric | Value | Notes |
|--------|-------|-------|
| **Average Transcription Accuracy** | 88.2% | Quiet room conditions |
| **Grammar Error Detection** | 88% | Manual validation baseline |
| **Articulation Error Detection** | 91% | Manual validation baseline |
| **Vocabulary Error Detection** | 85% | Manual validation baseline |
| **Average Practice Accuracy** | 76.6% | Across all sections |
| **Session Completion Rate** | 100% | All sessions completed |
| **Average Session Duration** | 10.5 min | Range: 8-12 minutes |

### Improvement Trends

```
Week 1 Progress (Pilot Data):
Session 1: 72% accuracy ‚Üí Session 6: 87% accuracy
Improvement: +15% over 6 sessions
```

### Error Type Distribution

| Error Type | Frequency | Common Examples |
|-----------|-----------|-----------------|
| Grammar | 45% | "have" ‚Üí "has", article usage |
| Articulation | 35% | "th" sounds, "s" sounds |
| Vocabulary | 20% | "thing" ‚Üí specific noun |

### Session Metrics (Sample from JSON)

```json
{
  "date": "2025-10-30",
  "subject": "dog",
  "initial_errors": 3,
  "sections_completed": 11,
  "total_attempts": 35,
  "average_accuracy": 76.58%
}
```

### Key Findings

‚úÖ **High Engagement**: Children completed all sessions willingly  
‚úÖ **Measurable Progress**: 12-15% accuracy improvement in 1 week  
‚úÖ **Error Detection**: 85-91% detection accuracy across types  
‚úÖ **User Experience**: Positive feedback from parents  
‚ö†Ô∏è **Limitations**: Small sample size, controlled environment  

---

## üõ†Ô∏è Technologies Used

### Core Technologies

| Technology | Version | Purpose |
|-----------|---------|---------|
| **Python** | 3.8+ | Core programming language |
| **Streamlit** | 1.28+ | Web application framework |
| **OpenAI Whisper** | Latest | Speech recognition (ASR) |
| **Google Gemini** | 2.5 Flash | Error detection & story generation |
| **gTTS** | 2.3+ | Text-to-speech synthesis |

### Libraries

```python
# Speech Processing
openai-whisper      # Speech-to-text
torch               # Deep learning framework
torchaudio          # Audio processing
librosa             # Audio analysis
soundfile           # Audio I/O

# AI/NLP
google-generativeai # Gemini API
transformers        # Model support

# Web App
streamlit           # UI framework
st-audiorec         # Audio recording widget

# Utilities
pandas              # Data manipulation
python-dotenv       # Environment management
jiwer               # WER calculation (future)
```

---

## üîí Ethics & Privacy

### Data Privacy Policy

This repository follows **safe-public** practices:

‚úÖ **What IS included:**
- Source code (fully open)
- Anonymized session metrics
- System architecture & documentation
- Sample placeholder data

‚ùå **What is NOT included:**
- Raw audio recordings of children
- Personal identifying information
- Parental consent forms (kept offline)
- Real session transcripts with names

### Ethical Guidelines

1. **Parental Consent**: Written consent required before use
2. **Data Anonymization**: All audio stripped of metadata
3. **Local Storage**: No cloud uploads without explicit permission
4. **Transparency**: Clear explanations of data usage
5. **Child Safety**: Age-appropriate content only

### For Researchers/Users

If you use this system with real children:

1. ‚úÖ Obtain **written informed consent** from parents/guardians
2. ‚úÖ Store audio in **secure, offline** locations
3. ‚úÖ Anonymize all data (rename files, remove metadata)
4. ‚úÖ Follow local **IRB/ethics** board requirements
5. ‚úÖ Use as **supplement**, not replacement for professional therapy

See [ETHICS.md](ETHICS.md) for complete policy.

---

## üîÆ Future Enhancements

### Planned Features

- [ ] **Real-time Processing**: Live transcription during recording
- [ ] **Multi-language Support**: Hindi, Tamil, Telugu, Bengali
- [ ] **Mobile Application**: iOS and Android apps
- [ ] **Pronunciation Scoring**: Detailed phoneme-level analysis
- [ ] **Gamification**: Points, badges, achievements
- [ ] **Parent Dashboard**: Detailed progress reports
- [ ] **Video Support**: Visual articulation guidance
- [ ] **Offline Mode**: No internet required
- [ ] **Cloud Sync**: Optional secure cloud backup
- [ ] **Group Sessions**: Multi-child practice mode

### Technical Improvements

- [ ] Fine-tune Whisper on Indian children's speech
- [ ] Implement custom phoneme recognition
- [ ] Add reinforcement learning for adaptive difficulty
- [ ] GPU optimization for faster processing
- [ ] Docker containerization
- [ ] CI/CD pipeline (GitHub Actions)
- [ ] Comprehensive unit tests
- [ ] Load testing and performance optimization

---

## ü§ù Contributing

We welcome contributions! Here's how you can help:

### Ways to Contribute

1. üêõ **Bug Reports**: Open an issue with detailed reproduction steps
2. üí° **Feature Requests**: Suggest new features or improvements
3. üìù **Documentation**: Improve README, add tutorials
4. üß™ **Testing**: Add unit tests, integration tests
5. üé® **UI/UX**: Design improvements, accessibility
6. üåç **Localization**: Add language support

### Development Setup

```bash
# Fork and clone
git clone https://github.com/roshnrf/storyweaver-speech-therapy.git
cd storyweaver-speech-therapy

# Create feature branch
git checkout -b feature/amazing-feature

# Make changes and test
streamlit run app.py

# Commit and push
git add .
git commit -m "Add amazing feature"
git push origin feature/amazing-feature

# Open Pull Request on GitHub
```

### Contribution Guidelines

- Follow PEP 8 style guide
- Add docstrings to functions
- Include unit tests for new features
- Update documentation
- Keep commits atomic and descriptive

---

## üìÑ License

This project is licensed under the **MIT License** - see the [LICENSE](LICENSE) file for details.

```
MIT License

Copyright (c) 2024 Roshan A Rauof, Reem Fariha

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction...
```

---

## üìß Contact

**Roshan A Rauof**  
üìß Email: roshanabdlrf03@gmail.com  
üîó LinkedIn: [linkedin.com/in/roshanarauof](https://www.linkedin.com/in/rosh003)  
üêô GitHub: [https://github.com/roshnrf](https://github.com/roshnrf)

**Reem Fariha**  
üìß Email: reem.fariha@example.com  
üîó LinkedIn: [linkedin.com/in/reemfariha](https://www.linkedin.com/in/reem-fariha-456509224/)  
üêô GitHub: [@reemfariha](https://github.com/reemfariha)

---

## üôè Acknowledgments

- **OpenAI** for the Whisper speech recognition model
- **Google** for Gemini API and gTTS service
- **Streamlit** team for the excellent framework
- **Parents and children** who participated in pilot testing
- **VIT University** for project support
- All open-source contributors

---

## üìö Citation

If you use this project in your research, please cite:

```bibtex
@software{storyweaver2024,
  author = {Roshan A Rauof, and Reem Fariha},
  title = {StoryWeaver: AI-Powered Speech Therapy for Children},
  year = {2024},
  publisher = {GitHub},
  url = {https://github.com/roshnrf/StoryWeaver},
  note = {Supervised by Dr. Anusooya G}
}
```

---

## ‚≠ê Star History

If you find this project helpful, please consider giving it a star! ‚≠ê

[![Star History Chart](https://api.star-history.com/svg?repos=roshnrf/StoryWeaver&type=Date)](https://star-history.com/#roshnrf/StoryWeaver&Date)

---

<div align="center">

**Made with ‚ù§Ô∏è for improving children's communication**

[‚¨Ü Back to Top](#-storyweaver-ai-powered-speech-therapy-for-children)


</div>















